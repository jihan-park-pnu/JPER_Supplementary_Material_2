# ==========================================================
# 1. ê¸°ë³¸ ì„¤ì •
# ==========================================================

import os
import re
from pathlib import Path
import requests
from openai import OpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from FlagEmbedding import FlagReranker
from PyPDF2 import PdfReader, PdfWriter

# ====== ê²½ë¡œ ì„¤ì • ======
BASE = Path(r"C:\Users\RDPL-005\Desktop")
PDF_DIR = BASE / "CCAP"              # ì›ë³¸ PDF í´ë”
OUT_DIR = BASE / "Parsed_Preview"    # Passer ê²°ê³¼ ì €ì¥ í´ë”
CHROMA_DIR = BASE / "Chroma_Index"   # RAGìš© Chroma ì¸ë±ìŠ¤ í´ë”
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ====== API í‚¤ ======
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY", "up_**********tslL") 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-proj-_1**********A")
LLM_MODEL = "gpt-5-mini-2025-08-07"

# ====== Embedding & DB ì„¸íŒ… ======
EMB_MODEL = "BAAI/bge-m3"
emb = HuggingFaceEmbeddings(model_name=EMB_MODEL)
db = Chroma(persist_directory=str(CHROMA_DIR), embedding_function=emb)

# ====== Reranker (ì—…ê·¸ë ˆì´ë“œ ë²„ì „) ======
reranker = FlagReranker("BAAI/bge-reranker-v2-m3", use_fp16=False)

print("[í™˜ê²½ ì„¤ì • ì™„ë£Œ]")
print(f"PDF í´ë”: {PDF_DIR}")
print(f"Chroma DB: {CHROMA_DIR}")
print(f"ëª¨ë¸: {LLM_MODEL}")
print(f"ì„ë² ë”©: {EMB_MODEL}")
print("ë¦¬ë­ì»¤: BAAI/bge-reranker-v2-m3")

# ==========================================================
# 2. Passer API ì§ì ‘ í˜¸ì¶œ (Upstage ë¬¸ì„œ íŒŒì‹±)
# ==========================================================
def parse_with_upstage(pdf_path: Path) -> str:
    url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}

    with open(pdf_path, "rb") as f:
        files = {"document": f}
        payload = {
            "model": "document-parse",         
            "ocr": "auto",
            "output_formats": ["html"],
            "merge_multipage_tables": True,
            "chart_recognition": True
        }

        print(f"[UPSTAGE] {pdf_path.name} ì—…ë¡œë“œ ì¤‘...")
        res = requests.post(url, headers=headers, files=files, data=payload)

    if res.status_code != 200:
        raise RuntimeError(
            f"âŒ Passer API ì˜¤ë¥˜ ({res.status_code})\n"
            f"ì‘ë‹µ: {res.text[:500]}..."
        )

    data = res.json()
    html = data.get("content", {}).get("html", "")
    if not html.strip():
        raise ValueError("âš ï¸ ê²°ê³¼ HTMLì´ ë¹„ì–´ ìˆìŒ")

    out_path = OUT_DIR / f"{pdf_path.stem}_parsed.html"
    out_path.write_text(html, encoding="utf-8")
    print(f"âœ… [Passer íŒŒì‹± ì™„ë£Œ] {out_path.name} ({len(html):,} chars)")

    return html

# ==========================================================
# 3. ëŒ€ìš©ëŸ‰ PDF ìë™ ë¶„í•  + Passer íŒŒì‹± + HTML ë³‘í•©
# ==========================================================
def list_pdfs(max_n: int = 10):
    """ğŸ“„ PDF í´ë” ë‚´ PDF íŒŒì¼ ëª©ë¡ ì¶œë ¥"""
    pdf_files = sorted(PDF_DIR.glob("*.pdf"))
    if not pdf_files:
        raise FileNotFoundError(f"âš ï¸ PDF ì—†ìŒ: {PDF_DIR}")

    print(f"ğŸ“„ {len(pdf_files)}ê°œ PDF ë°œê²¬:")
    for i, p in enumerate(pdf_files[:max_n], start=1):
        print(f"{i:>3}. {p.name}")
    return pdf_files

def pick_pdf_by_fragment(fragment: str) -> Path:
    """ğŸ¯ íŒŒì¼ëª… ì¼ë¶€(fragment)ë¡œ íŠ¹ì • PDF ì„ íƒ"""
    frag = fragment.lower()
    matches = [p for p in PDF_DIR.glob("*.pdf") if frag in p.name.lower()]
    if not matches:
        raise FileNotFoundError(f"'{fragment}'ì— í•´ë‹¹í•˜ëŠ” PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    matches.sort(key=lambda p: len(p.name))  # ê°€ì¥ ì§§ì€ ì´ë¦„ ìš°ì„ 
    target = matches[0]
    print(f"ğŸ¯ ì„ íƒëœ PDF: {target.name}")
    return target

def split_pdf(input_path: Path, output_dir: Path, max_pages: int = 90):
    reader = PdfReader(str(input_path))
    total_pages = len(reader.pages)
    parts = (total_pages // max_pages) + (1 if total_pages % max_pages else 0)
    output_paths = []

    print(f"ğŸ“˜ {input_path.name} ({total_pages} pages) â†’ {parts}ê°œë¡œ ë¶„í•  ì˜ˆì •")

    for i in range(parts):
        writer = PdfWriter()
        start, end = i * max_pages, min((i + 1) * max_pages, total_pages)
        for j in range(start, end):
            writer.add_page(reader.pages[j])

        out_path = OUT_DIR / f"{input_path.stem}_part{i+1}.pdf"
        with open(out_path, "wb") as f:
            writer.write(f)
        output_paths.append(out_path)
        print(f"âœ… {out_path.name} ì €ì¥ ({end - start}p)")

    return output_paths

def parse_large_pdf_with_upstage(pdf_path: Path, max_pages: int = 90) -> str:
    split_paths = split_pdf(pdf_path, OUT_DIR, max_pages=max_pages)
    merged_html = ""

    for i, part_path in enumerate(split_paths, start=1):
        print(f"\nğŸš€ [{i}/{len(split_paths)}] {part_path.name} íŒŒì‹± ì¤‘...")
        try:
            html_chunk = parse_with_upstage(part_path)
            merged_html += f"\n<!-- PART {i} START -->\n" + html_chunk + f"\n<!-- PART {i} END -->\n"
        except Exception as e:
            print(f"âš ï¸ {part_path.name} íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

    merged_path = OUT_DIR / f"{pdf_path.stem}_merged.html"
    merged_path.write_text(merged_html, encoding="utf-8")

    print(f"\nğŸ¯ ì „ì²´ ë³‘í•© ì™„ë£Œ â†’ {merged_path.name} ({len(merged_html):,} chars)")
    return merged_html

def extract_section_by_fixed_keywords(pdf_path: Path, min_page_threshold: int = 10, max_section_pages: int = 90) -> list[Path]:
    reader = PdfReader(pdf_path)
    start_page, end_page = None, None

    start_kw = "ë¶€ë¬¸ë³„ì„¸ë¶€ì‹œí–‰ê³„íš"
    end_kw = "ê³„íšì˜ì§‘í–‰ë°ê´€ë¦¬"

    print(f"ğŸ” '{pdf_path.name}'ì—ì„œ '{start_kw}' ~ '{end_kw}' êµ¬ê°„ íƒìƒ‰ ì¤‘...")

    # ==== 1. ì‹œì‘/ë í˜ì´ì§€ íƒìƒ‰ =====
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        text_clean = re.sub(r"\s+", "", text)

        if start_page is None and i > min_page_threshold and start_kw in text_clean:
            start_page = i
            print(f"âœ… ë³¸ë¬¸ ì‹œì‘ í‚¤ì›Œë“œ ë°œê²¬ (p.{i+1})")
        elif start_page is not None and end_kw in text_clean:
            end_page = i
            print(f"âœ… ì¢…ë£Œ í‚¤ì›Œë“œ ë°œê²¬ (p.{i+1})")
            break

    if start_page is None:
        raise ValueError(f"âŒ '{start_kw}' í‚¤ì›Œë“œë¥¼ {min_page_threshold+1}í˜ì´ì§€ ì´í›„ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if end_page is None:
        end_page = len(reader.pages)

    total_pages = end_page - start_page
    print(f"ğŸ“„ ì¶”ì¶œ êµ¬ê°„: p.{start_page+1}â€“{end_page} ({total_pages} pages)")

    # ==== 2. 90í˜ì´ì§€ ì´ˆê³¼ ì‹œ ìë™ ë¶„í•  =====
    section_parts = []
    for idx in range(0, total_pages, max_section_pages):
        writer = PdfWriter()
        part_start = start_page + idx
        part_end = min(start_page + idx + max_section_pages, end_page)
        for j in range(part_start, part_end):
            writer.add_page(reader.pages[j])

        part_path = OUT_DIR / f"{pdf_path.stem}_section_part{len(section_parts)+1}.pdf"
        with open(part_path, "wb") as f:
            writer.write(f)

        section_parts.append(part_path)
        print(f"âœ… ì„¹ì…˜ ë¶€ë¶„ ì €ì¥: {part_path.name} ({part_end - part_start}p)")

    print(f"ğŸ“˜ 'ë¶€ë¬¸ë³„ ì„¸ë¶€ì‹œí–‰ê³„íš' ì„¹ì…˜ {len(section_parts)}ê°œë¡œ ë¶„í•  ì™„ë£Œ")
    return section_parts

files = list_pdfs()
target_pdf = pick_pdf_by_fragment("ìˆœì²œì‹œ")

# â€˜ë¶€ë¬¸ë³„ ì„¸ë¶€ì‹œí–‰ê³„íšâ€™ ì„¹ì…˜ë§Œ ì¶”ì¶œ (ìë™ 90p ë‹¨ìœ„ ë¶„í• )
section_parts = extract_section_by_fixed_keywords(target_pdf, min_page_threshold=150, max_section_pages=90)

# ê° ë¶€ë¶„ì„ ìˆœì°¨ì ìœ¼ë¡œ Passerë¡œ íŒŒì‹± ë° ë³‘í•©
merged_html = ""
for i, section_pdf in enumerate(section_parts, start=1):
    print(f"\nğŸš€ [ì„¹ì…˜ {i}/{len(section_parts)}] Passer íŒŒì‹± ì¤‘...")
    html_chunk = parse_with_upstage(section_pdf)
    merged_html += f"\n<!-- SECTION {i} START -->\n" + html_chunk + f"\n<!-- SECTION {i} END -->\n"

merged_path = OUT_DIR / f"{target_pdf.stem}_section_merged.html"
merged_path.write_text(merged_html, encoding="utf-8")

print(f"\nâœ… ì „ì²´ ì„¹ì…˜ ë³‘í•© ì™„ë£Œ â†’ {merged_path.name}")
print(f"ğŸ“‘ ì´ {len(section_parts)}ê°œ ì„¹ì…˜ ë³‘í•© ì™„ë£Œ ({len(merged_html):,} chars)")

# ==========================================================
# 4. LLM ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ (ì •ë³´ ì¶”ì¶œ ë‹¨ê³„)
# ==========================================================
def ask_llm_on_document(html_path: Path, model: str = LLM_MODEL) -> str:
    client = OpenAI(api_key=OPENAI_API_KEY)
    document_text = html_path.read_text(encoding="utf-8")
    print(f"ë¬¸ì„œ ê¸¸ì´: {len(document_text):,} chars â†’ ë‹¨ì¼ ë¶„ì„ ì‹œì‘")

    # Prompt
    user_prompt = f"""
        # Extract the text exactly as written in the document. Do not paraphrase, rewrite, or modify wording.
        # Do not infer, assume, or supplement any information that is not explicitly stated in the document.
        # Exclude all labels, numbering, bracketed codes, and formatting markers from outputs; return only the descriptive text.
        
        # Objective/Action classification rules:       
        ## Only the sections in the documentâ€™s tables or main text that are explicitly marked as â€œì¶”ì§„ì „ëµâ€ should be identified as {{objective}}.
        ## Only the sections in the documentâ€™s tables or main text that are explicitly marked as â€œì‹¤ì²œê³¼ì œâ€ should be identified as {{action}}.
        ## Only structural labels (e.g., ì¶”ì§„ì „ëµ, ì‹¤ì²œê³¼ì œ) determine classification; Do not classify objectives and actions based on semantic meaning, wording style, and phrasing.
        
        # Maladaptation definition: 
        ## Maladaptation arises from unintended trade-offs created by implementing an action to achieve its objectiveâ€”such as harms imposed on other policy goals, social groups, or spatial areas.
        ## Do not classify background problems, general negative conditions, or implementation challenges (e.g., costs, burdens, resource shortages) as maladaptation.
        ## Do not infer maladaptation unless explicitly stated; if no maladaptation is explicitly mentioned for an action, output â€œ(Missing)â€.
        
        # Extract maladaptation risks only for each {{action}} in relation to its corresponding {{objective}}.
        # Attach maladaptation output under each action, but treat maladaptation as occurring at the objectiveâ€“action pair level.
        
        Output Format (must follow this structure strictly):
        # Objective: {{objective}}
        ## Action: {{action}}
        ## Maladaptation risks: ...
        (Repeat this block for all objectives)
        
        === DOCUMENT START ===
        {document_text}
        === DOCUMENT END ===
    """

    resp = client.responses.create(
        model=model,
        input=[{"role": "user", "content": user_prompt}],
    )

    final_text = resp.output_text.strip() if hasattr(resp, "output_text") else ""
    clean_stem = html_path.stem.replace("_section_merged", "")
    out_path = OUT_DIR / f"LLM_Extract_{clean_stem}.txt"
    out_path.write_text(final_text, encoding="utf-8")    

    print(f"[ë¶„ì„ ì™„ë£Œ] {out_path.name}")
    return final_text

html_path = OUT_DIR / f"{target_pdf.stem}_section_merged.html"
ask_llm_on_document(html_path)

# ==========================================================
# 5. Inference stage: Objective-action pair inference
# ==========================================================
def infer_missing_impacts(extracted_txt_path: Path, model: str = LLM_MODEL, top_k: int = 5):
    client = OpenAI(api_key=OPENAI_API_KEY)
    extracted_text = extracted_txt_path.read_text(encoding="utf-8")

    print(f"ì¶”ë¡  ëŒ€ìƒ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜´: {extracted_txt_path.name}")

    # === 1. Objective ë¸”ë¡ ë¶„ë¦¬ ===
    sections = re.split(r"(?=^# Objective:)", extracted_text, flags=re.MULTILINE)
    sections = [s.strip() for s in sections if s.strip()]
    print(f"ì´ {len(sections)}ê°œ Objective ë¸”ë¡ íƒì§€")

    total_results = []

    for sidx, sec in enumerate(sections, start=1):

        # --- Objective ì¶”ì¶œ ---
        obj_match = re.search(r"^# Objective:\s*(.+)", sec, flags=re.MULTILINE)
        if not obj_match:
            continue
        objective = obj_match.group(1).strip()

        # --- Action + Risk ì¶”ì¶œ ---
        pairs = re.findall(
            r"## Action:\s*(.+?)\s*## Maladaptation risks:\s*(.+?)(?=^## Action:|$)",
            sec,
            flags=re.MULTILINE | re.DOTALL,
        )

        if not pairs:
            print(f"[ê²½ê³ ] Actionâ€“Risk ìŒ ì—†ìŒ â†’ {objective} ê±´ë„ˆëœ€")
            continue

        # Missingì¸ actionë§Œ ëª¨ìœ¼ê¸°
        missing_actions = []
        for action, risk in pairs:
            action = action.strip()
            risk = risk.strip()
            if risk == "(Missing)":
                missing_actions.append(action)

        if not missing_actions:
            continue

        # === 2. Actionë³„ inference ===
        for aidx, action in enumerate(missing_actions, start=1):

            print(f"\n Objective ì¶”ë¡  ì‹œì‘ ({sidx}/{len(sections)}) â€” {objective}")
            print(f"   Action {aidx}/{len(missing_actions)}: {action}")

            # --- Query ìƒì„± (Objectiveâ€“Measure pair) ---
            query = f"Maladaptation from implementing '{objective}' via measure '{action}'."

            # --- Retrieval ---
            docs = db.similarity_search(query, k=top_k)

            # --- Rerank ---
            scores = [reranker.compute_score([query, d.page_content]) for d in docs]
            reranked_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
            top_docs = [doc for _, doc in reranked_docs[:3]]

            # --- Evidence êµ¬ì„± ---
            if top_docs:
                context_chunks = []
                for i, d in enumerate(top_docs):
                    src = d.metadata.get("source", "unknown")
                    title = d.metadata.get("title", "")
                    context_chunks.append(
                        f"[{i+1}] {src if src else title}:\n{d.page_content.strip()}"
                    )
                context_text = "\n\n".join(context_chunks)
            else:
                context_text = "(No evidence)"
            
            # --- LLM í”„ë¡¬í”„íŠ¸ ---
            prompt = f"""
                     # Your task is to infer maladaptation that may arise when achieving the given {objective} through its {action}.

                     # Maladaptation definition: 
                     ## Maladaptation arises from unintended trade-offs created by implementing an action to achieve its objectiveâ€”such as harms imposed on other policy goals, social groups, or spatial areas.
                     ## Do not classify background problems, general negative conditions, or implementation challenges (e.g., costs, burdens, resource shortages) as maladaptation.
                 
                     # Using only the contextual evidence provided below, infer maladaptation risks for each objectiveâ€“action pair.
                     # If no evidence supports a maladaptation, write: "(No evidence-based maladaptation found)"
                 
                     ---
                     Objective: {objective}
                    
                     Actions: {action}

                     Contextual Evidence: {context_text}
                     ---
                 
                     # Instructions:
                     ## 1. Write ONE concise paragraph describing an evidence-supported maladaptation.
                     ## 2. End with an inline citation: (Evidence: Author, Year).
                     ## 3. If no evidence supports a maladaptation risk, output only: "(No evidence-based maladaptation found)"
                     ## 4. Respond in English.
                        
                     Output format:
                     # Inferred risk for: {objective} â€“ {action}
                     [Paragraph OR (No evidence-based maladaptation found)]
                     """

            # --- LLM í˜¸ì¶œ ---
            resp = client.responses.create(
                model=model,
                input=[{"role": "user", "content": prompt}],
            )

            inference = resp.output_text.strip() if hasattr(resp, "output_text") else "(Inference failed)"
            total_results.append(inference + "\n")

    # === 3. ê²°ê³¼ ì €ì¥ ===
    if not total_results:
        print("ëª¨ë“  í•­ëª©ì— Adverse Impactsê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ë¡  ë¶ˆí•„ìš”.")
        return None

    final_output = "\n\n".join(total_results)

    clean_stem = extracted_txt_path.stem.replace("_SINGLE", "").replace("_section_merged", "").replace("LLM_Extract_", "")
    out_path = OUT_DIR / f"LLM_Inferred_{clean_stem}.txt"
    out_path.write_text(final_output, encoding="utf-8")

    print(f"\nì¶”ë¡  ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {out_path.name}")
    return final_output

extracted_txt_path = OUT_DIR / f"LLM_Extract_{html_path.stem.replace('_section_merged','')}.txt"
infer_missing_impacts(extracted_txt_path)